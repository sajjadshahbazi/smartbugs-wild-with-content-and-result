import json
import re
import os
from pathlib import Path
from imblearn.over_sampling import SMOTE
import pandas as pd
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import Sequence
import sys
from gensim.models import Word2Vec
import numpy as np
import pickle
import PreProcessTools
import numpy as np
import io
from tensorflow.keras import backend as K
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dropout, Dense
from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dropout, Dense
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from tensorflow.python.platform import build_info as tf_build_info
from tensorflow.keras.layers import Input
from tensorflow.keras.layers import Conv2D, Cropping2D, MaxPooling2D, UpSampling2D, concatenate, Flatten, Dense, Bidirectional, LSTM, Input, Reshape, BatchNormalization, Reshape
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, accuracy_score




duration_stat = {}
count = {}
output = {}
safe_count = 0
vul_count = 0
labels = []
fragment_contracts = []
dataframes_list = []
batch_size = 1000  # Ú©Ø§Ù‡Ø´ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¯Ø³ØªÙ‡ Ø¨Ù‡ 500 Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯
output_name = 'icse20'
vector_length = 300
tool_stat = {}
tool_category_stat = {}
total_duration = 0
contract_vulnerabilities = {}
sequence_length = 10
vulnerability_mapping = {}

tools = ['mythril', 'slither', 'osiris', 'smartcheck', 'manticore', 'maian', 'securify',
         'honeybadger']  # all tools analizer

target_vulnerability_integer_overflow = 'Integer Overflow'  # sum safe smart contract: 28953, sum vulnarable smart contract: 18445
target_vulnerability_reentrancy = 'Reentrancy'  # sum safe smart contract: 38423, sum vulnarable smart contract: 8975
target_vulnerability_transaction_order_dependence = 'Transaction order dependence'  # sum safe smart contract: 45380, sum vulnarable smart contract: 2018
target_vulnerability_timestamp_dependency = 'timestamp'  # sum safe smart contract: 45322 , sum vulnarable smart contract: 2076
target_vulnerability_callstack_depth_attack = 'Depth Attack'  # sum safe smart contract: 45380 , sum vulnarable smart contract: 2018
target_vulnerability_integer_underflow = 'Integer Underflow'  # sum safe smart contract: 43727 , sum vulnarable smart contract: 3671

target_vulner = target_vulnerability_reentrancy


# ROOT = os.path.realpath(os.path.join(os.path.dirname(__file__), '..'))
# CACHE_DIR = os.path.join(ROOT, 'vectorcollections')

ROOT = '/content/smartbugs-wild-with-content-and-result' # Linux
CACHE_DIR = os.path.join(ROOT, 'vectorcollections') # Linux


cache_path = os.path.join(CACHE_DIR, 'tokenized_fragments.pkl')
vulnerability_fd = open(os.path.join(ROOT, 'metadata', 'vulnerabilities.csv'), 'w', encoding='utf-8')

# PATH = f"{ROOT}\\contracts\\"  # main data set
# PATH = f"{ROOT}\\contract\\"  # part of main data set
# PATH = f"{ROOT}\\contra\\"  # one smart contract

PATH = os.path.join(ROOT, 'contracts') # linux
os.chdir(PATH)

final_df = pd.DataFrame(columns=['X', 'Y'])


def focal_loss(alpha=0.25, gamma=2.0):
    def loss(y_true, y_pred):
        epsilon = K.epsilon()  # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² log(0)
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)
        pt = y_true * y_pred + (1 - y_true) * (1 - y_pred)  # Ø§Ø­ØªÙ…Ø§Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ØµØ­ÛŒØ­
        return -K.mean(alpha * K.pow(1. - pt, gamma) * K.log(pt))  # ÙØ±Ù…ÙˆÙ„ Focal Loss

    return loss


def is_sentence_in_text(sentence, text):
    sentence = sentence.lower()
    text = text.lower()
    text = re.sub(r'[^a-z ]', '', text)
    flg = sentence in text
    return flg

def load_batches(folder, file_extension=".pkl"):
    X_batches, Y_batches = [], []
    print(f"========== {folder}")
    for file in os.listdir(folder):
        if file.endswith(file_extension):
            with open(os.path.join(folder, file), 'rb') as f:
                X, Y = pickle.load(f)
                X_batches.append(X)
                Y_batches.append(Y)
    return np.vstack(X_batches), np.hstack(Y_batches)


def getResultVulnarable(contract_name, target_vulnerability):

    total_duration = 0
    res = False
    lines = []
    for tool in tools:
        # path_result = os.path.join(f"{ROOT}\\results\\", tool, output_name, contract_name, 'result.json')
        path_result = os.path.join(f"{ROOT}results", tool, output_name, contract_name, 'result.json') # Linux
        if not os.path.exists(path_result):
            continue
        with open(path_result, 'r', encoding='utf-8') as fd:
            data = None
            try:
                data = json.load(fd)
            except Exception as a:
                continue
            if tool not in duration_stat:
                duration_stat[tool] = 0
            if tool not in count:
                count[tool] = 0
            count[tool] += 1
            duration_stat[tool] += data['duration']
            total_duration += data['duration']
            if contract_name not in output:
                output[contract_name] = {
                    'tools': {},
                    'lines': set(),
                    'nb_vulnerabilities': 0
                }
            output[contract_name]['tools'][tool] = {
                'vulnerabilities': {},
                'categories': {}
            }
            if data['analysis'] is None:
                continue
            if tool == 'mythril':
                analysis = data['analysis']
                if analysis['issues'] is not None:
                    for result in analysis['issues']:
                        vulnerability = result['title'].strip()
                        if is_sentence_in_text(target_vulnerability, vulnerability):
                            res = True
                            lines.extend([result['lineno']])

            elif tool == 'oyente' or tool == 'osiris' or tool == 'honeybadger':
                for analysis in data['analysis']:
                    if analysis['errors'] is not None:
                        for result in analysis['errors']:
                            vulnerability = result['message'].strip()
                            if is_sentence_in_text(target_vulnerability, vulnerability):
                                res = True
                                lines.extend([result['line']])

            elif tool == 'manticore':
                for analysis in data['analysis']:
                    for result in analysis:
                        vulnerability = result['name'].strip()
                        if is_sentence_in_text(target_vulnerability, vulnerability):
                            res = True
                            lines.extend([result['line']])

            elif tool == 'maian':
                for vulnerability in data['analysis']:
                    if data['analysis'][vulnerability]:
                        if is_sentence_in_text(target_vulnerability, vulnerability):
                            res = True
                            # None lines

            elif tool == 'securify':
                for f in data['analysis']:
                    analysis = data['analysis'][f]['results']
                    for vulnerability in analysis:
                        for line in analysis[vulnerability]['violations']:
                            if is_sentence_in_text(target_vulnerability, vulnerability):
                                res = True
                                lines.extend([line + 1])

            elif tool == 'slither':
                analysis = data['analysis']
                for result in analysis:
                    vulnerability = result['check'].strip()
                    line = None
                    if 'source_mapping' in result['elements'][0] and len(
                            result['elements'][0]['source_mapping']['lines']) > 0:
                        line = result['elements'][0]['source_mapping']['lines']
                    if is_sentence_in_text(target_vulnerability, vulnerability):
                        if line is not None:
                            res = True
                            lines.extend(line)

            elif tool == 'smartcheck':
                analysis = data['analysis']
                for result in analysis:
                    vulnerability = result['name'].strip()
                    if is_sentence_in_text(target_vulnerability, vulnerability):
                        res = True
                        lines.extend([result['line']])

            elif tool == 'solhint':
                analysis = data['analysis']
                for result in analysis:
                    vulnerability = result['type'].strip()
                    if is_sentence_in_text(target_vulnerability, vulnerability):
                        res = True
                        lines.extend([int(result['line'])])

    return res, lines



SENSITIVE_OPERATORS_REETRANCY = ['call', 'delegatecall', 'send', 'transfer', 'selfdestruct']

def contains_sensitive_operator(function_body):
    for operator in SENSITIVE_OPERATORS_REETRANCY:
        if operator in function_body:
            return True
    return False


def save_to_file(data, file_prefix, cache_dir, batch_size, batch_index):
    os.makedirs(cache_dir, exist_ok=True)  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ù¾ÙˆØ´Ù‡ CACHE_DIR
    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]
        filename = f"{file_prefix}_batch_{batch_index}_{i // batch_size}.pkl"  # Ù†Ø§Ù…â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒØ´Ø¯Ù‡
        filepath = os.path.join(cache_dir, filename)
        with open(filepath, 'wb') as f:
            pickle.dump(batch, f)
        print(f"Saved batch to {filepath}")

def extract_functions(code):
    functions = []

    # Ø§Ù„Ú¯ÙˆÛŒ regex Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ÙØ§Ù†Ú©Ø´Ù†â€ŒÙ‡Ø§
    function_pattern = re.compile(
        r'function\s+\w+\s*\(.*\)\s*(public|private|internal|external)*\s*(view|pure)*\s*(returns\s*\(.*\))?\s*{')

    matches = function_pattern.finditer(code)
    for match in matches:
        function_start = match.start()
        function_end = code.find('}', function_start) + 1

        if function_end != -1:
            functions.append(code[function_start:function_end])
    return functions


def tokenize_solidity_code(code):
    pattern = r'\b(?:function|returns|uint256|internal|constant|assert|return|require|if|else|for|while)\b|[=<>!*&|()+\-;/\}]|\b[a-zA-Z_][a-zA-Z0-9_]*\b'
    tokens = re.findall(pattern, code)
    return tokens

def normalize_variables(tokens):
    normalized_tokens = []
    for token in tokens:
        if re.match(r'[a-zA-Z_][a-zA-Z0-9_]*', token) and token not in ['function', 'returns', 'internal', 'constant', 'assert', 'return']:
            normalized_tokens.append('VAR')  # Ø¨Ù‡ Ø¬Ø§ÛŒ Ø§Ø³Ù… Ù…ØªØºÛŒØ±ØŒ 'VAR' Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        elif token in ['}', '{', '(', ')', '[', ']', '.', ';', ',', '+', '-', '=', '!', '?', ':']:
            normalized_tokens.append(token)
        elif token.strip() == '':
            continue
        else:
            normalized_tokens.append(token)
    return normalized_tokens

def extract_functions_with_bodies(contract_code):
    functions = []
    function_pattern = re.compile(
        r'function\s+\w+\s*\(.*?\)\s*(public|private|internal|external)?\s*(view|pure)?\s*(returns\s*\(.*?\))?\s*{')

    lines = contract_code.splitlines()  # ØªÙ‚Ø³ÛŒÙ… Ú©Ø¯ Ø¨Ù‡ Ø®Ø·ÙˆØ·
    open_brackets = 0
    in_function = False
    function_body = []
    start_line = 0

    for i, line in enumerate(lines):
        if not in_function:
            match = function_pattern.search(line)
            if match:
                in_function = True
                start_line = i + 1  # Ø«Ø¨Øª Ø´Ù…Ø§Ø±Ù‡ Ø®Ø· Ø´Ø±ÙˆØ¹
                function_body = [line]
                open_brackets = line.count('{') - line.count('}')
        else:
            function_body.append(line)
            open_brackets += line.count('{')
            open_brackets -= line.count('}')
            if open_brackets == 0:
                end_line = i + 1  # Ø«Ø¨Øª Ø´Ù…Ø§Ø±Ù‡ Ø®Ø· Ù¾Ø§ÛŒØ§Ù†
                functions.append({
                    'function_body': '\n'.join(function_body),
                    'start_line': start_line,
                    'end_line': end_line,
                    'label': 0
                })
                in_function = False

    return functions

def vectorize_tokens(tokens):
    word2vec_model = Word2Vec(sentences=[tokens], vector_size=vector_length, window=5, min_count=1, workers=4)
    embeddings = [
        word2vec_model.wv[word] if word in word2vec_model.wv else np.zeros(vector_length)
        for word in tokens
    ]
    embeddings = embeddings[:sequence_length] + [np.zeros(vector_length)] * max(0, sequence_length - len(embeddings))
    return np.array(embeddings, dtype='float32')


def label_functions_by_vulnerable_lines(functions, vulnerable_lines):
    for func in functions:
        if any(func['start_line'] <= line <= func['end_line'] for line in vulnerable_lines):
            func['label'] = 1


def process_batch_with_categorization(files, target_vulnerability, batch_size, batch_index):
    X_sensitive_negative, Y_sensitive_negative = [], []
    X_vulnerable, Y_vulnerable = [], []
    X_safe, Y_safe = [], []
    max_function_length = 50

    sc_files = [f for f in files if f.endswith(".sol")]
    print(f"cont {sc_files.__len__()}")
    for file in sc_files:
        with (open(file, encoding="utf8") as f):
            contract_content = f.read()
            functions = extract_functions_with_bodies(contract_content)
            name = Path(file).stem
            res, vulnerable_lines = getResultVulnarable(name, target_vulnerability)
            label_functions_by_vulnerable_lines(functions, vulnerable_lines)
            for func in functions:
                fragments = PreProcessTools.get_fragments(func['function_body'])
                label = func['label']
                func_vectors = []

                for fragment in fragments:
                    if fragment.strip():
                        tokens = tokenize_solidity_code(fragment)
                        if tokens:
                            vectors = vectorize_tokens(tokens)
                            func_vectors.extend(vectors)
                if func_vectors:
                    padded_function = pad_sequences([func_vectors], maxlen=max_function_length, padding='post', dtype='float32')[0]
                    # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ØªÙˆØ§Ø¨Ø¹

                    if label == 1:
                        X_vulnerable.append(padded_function)
                        Y_vulnerable.append(label)
                    else:
                        if contains_sensitive_operator(func['function_body']):
                            X_sensitive_negative.append(padded_function)
                            Y_sensitive_negative.append(label)
                        else:
                            X_safe.append(padded_function)
                            Y_safe.append(label)

    X_vulnerable = np.array(X_vulnerable, dtype='float32')
    Y_vulnerable = np.array(Y_vulnerable, dtype='int32')

    X_sensitive_negative = np.array(X_sensitive_negative, dtype='float32')
    Y_sensitive_negative = np.array(Y_sensitive_negative, dtype='int32')

    X_safe = np.array(X_safe, dtype='float32')
    Y_safe = np.array(Y_safe, dtype='int32')

    batch_file_vulnerable = os.path.join(CACHE_DIR, f"vulnerable_batch_{batch_index}.pkl")
    batch_file_sensitive_negative = os.path.join(CACHE_DIR, f"sensitive_negative_batch_{batch_index}.pkl")
    batch_file_safe = os.path.join(CACHE_DIR, f"safe_batch_{batch_index}.pkl")

    with open(batch_file_vulnerable, 'wb') as f:
        pickle.dump((X_vulnerable, Y_vulnerable), f)

    with open(batch_file_sensitive_negative, 'wb') as f:
        pickle.dump((X_sensitive_negative, Y_sensitive_negative), f)

    with open(batch_file_safe, 'wb') as f:
        pickle.dump((X_safe, Y_safe), f)
    print(f"Batch saved to {batch_file_vulnerable}, {batch_file_sensitive_negative}", {batch_file_safe})





# def prepare_data_for_unet(X):
#     """ ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø³Ù‡â€ŒØ¨Ø¹Ø¯ÛŒ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ U-Net """
#     return np.expand_dims(X, axis=-1)  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ (samples, sequence_length, vector_length, 1)


def prepare_data_for_unet(X, target_shape=(50, 300)):
    """
    ØªØ¨Ø¯ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ø³Ù‡â€ŒØ¨Ø¹Ø¯ÛŒ Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ U-Net

    :param X: Ø¢Ø±Ø§ÛŒÙ‡ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ Ø´Ú©Ù„ (samples, sequence_length, vector_length)
    :param target_shape: Ø§Ø¨Ø¹Ø§Ø¯ Ù†Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ U-Net Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯ (Ø¨Ø§ÛŒØ¯ Ù‡Ù…â€ŒØ§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ø§ ÙˆØ±ÙˆØ¯ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§Ø´Ø¯)
    :return: Ø¢Ø±Ø§ÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø§ Ø§Ø¨Ø¹Ø§Ø¯ (samples, 50, 300, 1) Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± U-Net
    """
    if X.shape[1:] != target_shape:
        raise ValueError(f"âŒ Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ø¨Ø§ {target_shape} Ø³Ø§Ø²Ú¯Ø§Ø± Ù†ÛŒØ³Øª! Ø´Ú©Ù„ ÙØ¹Ù„ÛŒ: {X.shape}")

    # âœ… Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒÚ© Ø¨Ø¹Ø¯ Ú©Ø§Ù†Ø§Ù„ Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ U-Net
    X = np.expand_dims(X, axis=-1)  # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ (samples, sequence_length, vector_length, 1)

    print("\nğŸ” **Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ U-Net:**")
    print("ğŸ”¹ Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ X Ø¨Ø±Ø§ÛŒ U-Net:", X.shape)
    print("ğŸ”¹ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ù…Ù‚Ø¯Ø§Ø± X:", np.max(X))
    print("ğŸ”¹ Ú©Ù…ÛŒÙ†Ù‡ Ù…Ù‚Ø¯Ø§Ø± X:", np.min(X))
    print("ğŸ”¹ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± X:", np.mean(X))

    return X

def build_unet(input_shape):
    """ U-Net Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡ Ø¨Ø§ BatchNormalization Ùˆ ØªØ³Øª Ø®Ø±ÙˆØ¬ÛŒ """
    inputs = Input(input_shape)

    conv1 = Conv2D(64, (3, 5), activation='relu', padding='same')(inputs)
    conv1 = BatchNormalization()(conv1)
    pool1 = MaxPooling2D((1, 2), padding='same')(conv1)

    conv2 = Conv2D(128, (3, 7), activation='relu', padding='same')(pool1)
    conv2 = BatchNormalization()(conv2)
    pool2 = MaxPooling2D((1, 2), padding='same')(conv2)

    conv3 = Conv2D(256, (3, 7), activation='relu', padding='same')(pool2)

    up1 = UpSampling2D((1, 2))(conv3)
    concat1 = concatenate([conv2, up1])
    conv4 = Conv2D(128, (3, 7), activation='relu', padding='same')(concat1)

    up2 = UpSampling2D((1, 2))(conv4)
    concat2 = concatenate([conv1, up2])
    conv5 = Conv2D(64, (3, 5), activation='relu', padding='same')(concat2)

    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv5)

    return Model(inputs, outputs)

def build_unet_lstm(input_shape_unet, input_shape_lstm):
    """ ØªØ±Ú©ÛŒØ¨ U-Net Ùˆ LSTM Ø¨Ø§ ØªØ³Øª Ø®Ø±ÙˆØ¬ÛŒ U-Net """
    unet_model = build_unet(input_shape_unet)

    # **âœ… ØªØ³Øª Û±: Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø±ÙˆØ¬ÛŒ U-Net**
    sample_output = unet_model.predict(np.random.rand(5, 50, 300, 1))  # ØªØ³Øª Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡ ØªØµØ§Ø¯ÙÛŒ
    print("\nğŸ” **ØªØ³Øª Ø®Ø±ÙˆØ¬ÛŒ U-Net:**")
    print("ğŸ”¹ Ø´Ú©Ù„ Ø®Ø±ÙˆØ¬ÛŒ:", sample_output.shape)
    print("ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø± Ø¨ÛŒØ´ÛŒÙ†Ù‡:", np.max(sample_output))
    print("ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø± Ú©Ù…ÛŒÙ†Ù‡:", np.min(sample_output))
    print("ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†:", np.mean(sample_output))

    # ØªØ¨Ø¯ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ U-Net Ø¨Ù‡ ÙØ±Ù…Øª Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ LSTM
    lstm_input = Reshape((50, 300))(unet_model.output)

    lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(lstm_input)
    lstm_layer = Bidirectional(LSTM(64))(lstm_layer)

    dense1 = Dense(128, activation='relu')(lstm_layer)
    dense2 = Dense(64, activation='relu')(dense1)
    outputs = Dense(1, activation='sigmoid')(dense2)

    return Model(inputs=[unet_model.input], outputs=outputs)

def train_unet_lstm():
    X, Y = load_batches(CACHE_DIR, file_extension=".pkl")

    # **âœ… ØªØ³Øª Û²: Ø¨Ø±Ø±Ø³ÛŒ `NaN` Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ø«Ø§Ø¨Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§**
    print("\nğŸ” **ØªØ³Øª NaN Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ø«Ø§Ø¨Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:**")
    print("ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ NaN Ø¯Ø± X:", np.isnan(X).sum())
    print("ğŸ”¹ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯ Ø¯Ø± X:", np.unique(X))

    X_unet = prepare_data_for_unet(X, target_shape=(50, 300))

    X_train_lstm, X_test_lstm, X_train_unet, X_test_unet, Y_train, Y_test = train_test_split(
        X, X_unet, Y, test_size=0.2, random_state=42
    )

    model = build_unet_lstm((50, 300, 1), (X.shape[1], X.shape[2]))
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss="binary_crossentropy",
        metrics=['accuracy']
    )

    history = model.fit(
        [X_train_unet], Y_train,
        epochs=50, batch_size=32, validation_split=0.2, verbose=2
    )

    plt.figure(figsize=(10, 6))
    plt.plot(history.history['accuracy'], label='Train Accuracy', color='blue')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='orange')
    plt.plot(history.history['loss'], label='Train Loss', color='red')
    plt.plot(history.history['val_loss'], label='Validation Loss', color='green')
    plt.title('Training and Validation Metrics')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy / Loss')
    plt.legend()
    plt.grid()
    plt.savefig("training_plot_unet_lstm.png", dpi=300, bbox_inches='tight')
    plt.show()

    # **âœ… ØªØ³Øª Û³: Ø¨Ø±Ø±Ø³ÛŒ ØªØºÛŒÛŒØ±Ø§Øª `accuracy` Ùˆ `loss` Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù†**
    print("\nğŸ” **ØªØ³Øª ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± `accuracy` Ùˆ `loss`**")
    print("ğŸ”¹ Ø¯Ù‚Øª Ù†Ù‡Ø§ÛŒÛŒ:", history.history['accuracy'][-1])
    print("ğŸ”¹ Ø¯Ù‚Øª Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:", history.history['val_accuracy'][-1])
    print("ğŸ”¹ Ø®Ø·Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:", history.history['loss'][-1])
    print("ğŸ”¹ Ø®Ø·Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ:", history.history['val_loss'][-1])

    # **âœ… ØªØ³Øª Û´: Ø¨Ø±Ø±Ø³ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø¯Ù„**
    Y_pred = (model.predict([X_test_unet]) > 0.5).astype("int32")
    accuracy = accuracy_score(Y_test, Y_pred)
    report = classification_report(Y_test, Y_pred, target_names=['Safe', 'Vulnerable'], labels=[0, 1])

    print("\nğŸ” **ØªØ³Øª Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø¯Ù„:**")
    print(f"ğŸ”¹ Ø¯Ù‚Øª Ù†Ù‡Ø§ÛŒÛŒ Ù…Ø¯Ù„: {accuracy}")
    print("ğŸ”¹ Ú¯Ø²Ø§Ø±Ø´ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ:")
    print(report)

    model.save('final_unet_lstm_model.h5')
    print("\nâœ… **Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯!**")




if __name__ == "__main__":
    # files = [os.path.join(PATH, f) for f in os.listdir(PATH) if f.endswith(".sol")]
    # print(f"size files {files.__len__()}")
    # for batch_index, i in enumerate(range(0, len(files), batch_size)):
    #     batch_files = files[i:i + batch_size]
    #     print(f"size batch_files {batch_files.__len__()}")
    #     process_batch_with_categorization(batch_files, target_vulner, batch_size, batch_index)


    train_unet_lstm()




# ØªØ­Ù„ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„ LSTM-UNet
#
# âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ U-Net Ù…Ø¹ØªØ¨Ø± Ù‡Ø³ØªÙ†Ø¯.
# âœ… Ù…Ù‚Ø¯Ø§Ø± NaN Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ØµÙØ± Ø§Ø³ØªØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ù…Ø´Ú©Ù„ÛŒ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§ÙˆÙ„ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.
# âœ… Ø´Ú©Ù„ Ø®Ø±ÙˆØ¬ÛŒ U-Net (50, 300, 1) Ø§Ø³Øª Ùˆ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨ÛŒÙ† 0.47 ØªØ§ 0.52 Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯ØŒ Ø¨Ù†Ø§Ø¨Ø±Ø§ÛŒÙ† Ø®Ø±ÙˆØ¬ÛŒ U-Net Ø¯Ø± Ø­Ø§Ù„ ØªØºÛŒÛŒØ± Ø§Ø³Øª.
# ğŸ“‰ Ù…Ø´Ú©Ù„ Ø§ØµÙ„ÛŒ: Ú©Ø§Ù‡Ø´ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¯Ø± Ø·ÙˆÙ„ epochÙ‡Ø§
# ğŸ“Œ Ù…Ø±Ø­Ù„Ù‡ Û±: Ø¯Ù‚Øª Ø§ÙˆÙ„ÛŒÙ‡ Ø®ÙˆØ¨ØŒ ÙˆÙ„ÛŒ Ø¨Ù‡ Ø³Ø±Ø¹Øª Ú©Ø§Ù‡Ø´ Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯
#
#     Ø¯Ø± epoch 1 Ø¯Ù‚Øª train accuracy = 0.7434 Ùˆ Ø¯Ù‚Øª val accuracy = 0.5429 Ø§Ø³Øª.
#     ğŸ”¹ Ø§ÛŒÙ† Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ù…Ø¯Ù„ Ø¯Ø± Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ Ù†Ø³Ø¨ØªØ§Ù‹ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø±Ø¯ØŒ Ø§Ù…Ø§ Ø¹Ø¯Ù… ØªØ¹Ø§Ø¯Ù„ Ø¨ÛŒÙ† train Ùˆ val Ù…Ø´Ù‡ÙˆØ¯ Ø§Ø³Øª.
#
#     Ø¯Ø± epoch 2 Ø¯Ù‚Øª train accuracy = 0.7818 Ùˆ val accuracy = 0.6887 Ø§Ø³Øª.
#     ğŸ”¹ Ø§ÛŒÙ† ÛŒØ¹Ù†ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡ØŒ ÙˆÙ„ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù‡Ù†ÙˆØ² Ú©Ø§Ù…Ù„Ø§Ù‹ Ù‡Ù…Ú¯Ø§Ù… Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.
#
# ğŸ“Œ Ù…Ø±Ø­Ù„Ù‡ Û²: Ú¯ÛŒØ± Ú©Ø±Ø¯Ù† Ø¯Ø± Ù…Ù‚Ø¯Ø§Ø± Ø®Ø§Øµ (plateau)
#
#     Ø§Ø² epoch 3 Ø¨Ù‡ Ø¨Ø¹Ø¯ØŒ Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ù…Ù‚Ø¯Ø§Ø± 0.6887 Ú¯ÛŒØ± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.
#     Ù‡Ù…Ú†Ù†ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± loss Ú©Ø§Ù‡Ø´ Ù†Ù…ÛŒâ€ŒÛŒØ§Ø¨Ø¯ Ú©Ù‡ Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ù…Ø¯Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±Ø§ Ù…ØªÙˆÙ‚Ù Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.
#     train loss Ø­Ø¯ÙˆØ¯ 0.624 Ùˆ val loss Ø­Ø¯ÙˆØ¯ 0.620 Ø¨Ø§Ù‚ÛŒ Ù…Ø§Ù†Ø¯Ù‡â€ŒØ§Ù†Ø¯.
#
# ğŸ“Œ Ø¯Ù„Ø§ÛŒÙ„ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ù…Ø´Ú©Ù„ Ùˆ Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ
# âœ… Û±. Ø¢ÛŒØ§ U-Net Ø§ØµÙ„Ø§Ù‹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙÛŒØ¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ
#
# ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø®Ø±ÙˆØ¬ÛŒ U-Net 0.5003 Ø§Ø³ØªØŒ ÛŒØ¹Ù†ÛŒ Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ù…Ù‚Ø¯Ø§Ø± Ø®Ø±ÙˆØ¬ÛŒ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ Ø«Ø§Ø¨Øª Ø§Ø³Øª.
# ğŸ”¹ Ø¨Ù‡ Ø§ÛŒÙ† Ù…Ø¹Ù†ÛŒ Ú©Ù‡ U-Net ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ùˆ Ù…ØªÙØ§ÙˆØªÛŒ Ø±Ø§ ÛŒØ§Ø¯ Ù†Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
# ğŸ“Œ Ø±Ø§Ù‡â€ŒØ­Ù„:
#
#     Ù…ÛŒâ€ŒØªÙˆØ§Ù† ØªØ¹Ø¯Ø§Ø¯ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ U-Net Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯ ØªØ§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ù‚ÙˆÛŒâ€ŒØªØ± Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯.
#     Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡ Ú©Ø±Ù†Ù„â€ŒÙ‡Ø§ÛŒ U-Net Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯ ØªØ§ Ø§Ø±ØªØ¨Ø§Ø·Ø§Øª Ù…Ø­Ù„ÛŒ Ø¨Ù‡ØªØ±ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†Ø¯.
#
# âœ… Û². LSTM Ú†Ø·ÙˆØ± Ø±ÙØªØ§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŸ
#
# ğŸ”¹ Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù† U-NetØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ LSTM Ø§ÙØª Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ø¯Ù‚Øª Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± 0.68 Ø«Ø§Ø¨Øª Ù…Ø§Ù†Ø¯Ù‡ Ø§Ø³Øª.
# ğŸ”¹ Ø´Ø§ÛŒØ¯ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø§ÛŒÙ† Ø¨Ø§Ø´Ø¯ Ú©Ù‡ U-Net ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¨Ø§ Ø³Ø§Ø®ØªØ§Ø± LSTM Ù†Ø§Ø³Ø§Ø²Ú¯Ø§Ø± Ø§Ø³Øª.
# ğŸ“Œ Ø±Ø§Ù‡â€ŒØ­Ù„:
#
#     Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø¹Ø¯ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ U-Net ÛŒÚ© Flatten() Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯ ØªØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨Ù‡ Ø´Ú©Ù„ Ø®Ø·ÛŒ Ø¨Ù‡ LSTM Ø¨Ø±ÙˆØ¯.
#     Ø´Ø§ÛŒØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Dropout Ø¯Ø± LSTM Ú©Ù…Ú© Ú©Ù†Ø¯ ØªØ§ Ù…Ø¯Ù„ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØºÛŒØ± Ù…ÙÛŒØ¯ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±Ø¯.
#
# âœ… Û³. Ø¢ÛŒØ§ Ù…Ø¯Ù„ Ø¯Ú†Ø§Ø± vanishing gradient Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ
#
# ğŸ”¹ Ø§Ú¯Ø± Ú¯Ø±Ø§Ø¯ÛŒØ§Ù†â€ŒÙ‡Ø§ Ø¨Ù‡â€ŒØ¯Ø±Ø³ØªÛŒ Ø§Ù†ØªØ´Ø§Ø± Ù¾ÛŒØ¯Ø§ Ù†Ú©Ù†Ù†Ø¯ØŒ Ù…Ø¯Ù„ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ú¯ÛŒØ± Ú©Ù†Ø¯ Ùˆ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø±Ø§ Ù…ØªÙˆÙ‚Ù Ú©Ù†Ø¯.
# ğŸ“Œ Ø±Ø§Ù‡â€ŒØ­Ù„:
#
#     Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† BatchNormalization Ø¨Ø¹Ø¯ Ø§Ø² Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ U-Net
#     Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² activation='tanh' Ø¯Ø± LSTM Ø¨Ù‡â€ŒØ¬Ø§ÛŒ relu
#
# ğŸ“Œ Ú¯Ø§Ù… Ø¨Ø¹Ø¯ÛŒ: Ú†Ù‡ Ú©Ø§Ø± Ú©Ù†ÛŒÙ…ØŸ
#
# ğŸš€ âœ… Û±. Ø®Ø±ÙˆØ¬ÛŒ U-Net Ø±Ø§ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†:
#
#     Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± min Ùˆ maxØŒ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ U-Net Ø±Ø§ Ø¨ØµØ±ÛŒâ€ŒØ³Ø§Ø²ÛŒ Ú©Ù† ØªØ§ Ø¨Ø¨ÛŒÙ†ÛŒÙ… Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ú†Ù‡ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯.
#
# ğŸš€ âœ… Û². ØªØºÛŒÛŒØ±Ø§Øª Ú©ÙˆÚ†Ú© Ø¯Ø± U-Net Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†:
#
#     Ú©Ø±Ù†Ù„ 5x5 Ø¨Ù‡ Ø¬Ø§ÛŒ 3x3
#     BatchNormalization Ø¨Ø¹Ø¯ Ø§Ø² Ù‡Ø± Conv2D
#
# ğŸš€ âœ… Û³. ØªØ³Øª Ú©Ù† Ú©Ù‡ Ø§Ú¯Ø± U-Net Ø­Ø°Ù Ø´ÙˆØ¯ØŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ØªØ± Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŸ
#
#     Ù…Ø¯Ù„ Ø±Ø§ ÙÙ‚Ø· Ø¨Ø§ LSTM Ø³Ø§Ø¯Ù‡ Ø§Ø¬Ø±Ø§ Ú©Ù† Ùˆ Ø¨Ø¨ÛŒÙ† Ø¢ÛŒØ§ Ø¨Ù‡Ø¨ÙˆØ¯ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ÛŒØ§ Ø®ÛŒØ±.
#
#
# ğŸ” **ØªØ³Øª NaN Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ø«Ø§Ø¨Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:**
# ğŸ”¹ ØªØ¹Ø¯Ø§Ø¯ NaN Ø¯Ø± X: 0
# ğŸ”¹ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…Ù†Ø­ØµØ±Ø¨Ù‡â€ŒÙØ±Ø¯ Ø¯Ø± X: [-0.00340875 -0.00340001 -0.00336363 ...  0.00340445  0.00342231
#   0.00343681]
#
# ğŸ” **Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ U-Net:**
# ğŸ”¹ Ø´Ú©Ù„ Ù†Ù‡Ø§ÛŒÛŒ X Ø¨Ø±Ø§ÛŒ U-Net: (60536, 50, 300, 1)
# ğŸ”¹ Ø¨ÛŒØ´ÛŒÙ†Ù‡ Ù…Ù‚Ø¯Ø§Ø± X: 0.0034368085
# ğŸ”¹ Ú©Ù…ÛŒÙ†Ù‡ Ù…Ù‚Ø¯Ø§Ø± X: -0.0034087515
# ğŸ”¹ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± X: 2.232679e-05
# 2025-01-31 08:07:52.187096: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.
# I0000 00:00:1738310872.189170   16552 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20967 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9
# WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
# I0000 00:00:1738310874.033674   16901 service.cc:148] XLA service 0x7a17f40043f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
# I0000 00:00:1738310874.033772   16901 service.cc:156]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9
# 2025-01-31 08:07:54.080637: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
# I0000 00:00:1738310874.174356   16901 cuda_dnn.cc:529] Loaded cuDNN version 90300
# 2025-01-31 08:07:56.468609: W external/local_xla/xla/tsl/framework/bfc_allocator.cc:306] Allocator (GPU_0_bfc) ran out of memory trying to allocate 25.15GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.
# I0000 00:00:1738310877.221917   16901 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.
# 1/1 â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 4s 4s/step
#
# ğŸ” **ØªØ³Øª Ø®Ø±ÙˆØ¬ÛŒ U-Net:**
# ğŸ”¹ Ø´Ú©Ù„ Ø®Ø±ÙˆØ¬ÛŒ: (5, 50, 300, 1)
# ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø± Ø¨ÛŒØ´ÛŒÙ†Ù‡: 0.529945
# ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø± Ú©Ù…ÛŒÙ†Ù‡: 0.47566083
# ğŸ”¹ Ù…Ù‚Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†: 0.50036794
# Epoch 1/50
# 1211/1211 - 331s - 273ms/step - accuracy: 0.7434 - loss: 0.5244 - val_accuracy: 0.5429 - val_loss: 0.7171
# Epoch 2/50
# 1211/1211 - 309s - 255ms/step - accuracy: 0.7818 - loss: 0.4438 - val_accuracy: 0.6887 - val_loss: 0.6225
# Epoch 3/50
# 1211/1211 - 309s - 255ms/step - accuracy: 0.7767 - loss: 0.4406 - val_accuracy: 0.6887 - val_loss: 0.6227
# Epoch 4/50
# 1211/1211 - 307s - 254ms/step - accuracy: 0.6848 - loss: 0.6245 - val_accuracy: 0.6887 - val_loss: 0.6207
# Epoch 5/50
# 1211/1211 - 308s - 254ms/step - accuracy: 0.6848 - loss: 0.6240 - val_accuracy: 0.6887 - val_loss: 0.6201
# Epoch 6/50
# 1211/1211 - 308s - 254ms/step - accuracy: 0.6848 - loss: 0.6239 - val_accuracy: 0.6887 - val_loss: 0.6202
# Epoch 7/50
# 1211/1211 - 307s - 254ms/step - accuracy: 0.6848 - loss: 0.6238 - val_accuracy: 0.6887 - val_loss: 0.6205
# Epoch 8/50







